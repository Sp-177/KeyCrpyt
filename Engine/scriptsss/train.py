"""
============================================================
üîê KeyCrypt ‚Äî GRU Base Model Trainer
Author: Shubham Patel (NIT Raipur)
============================================================

‚úÖ Loads preprocessed dataset: kaggle_strong_passwords.csv
‚úÖ Loads vocab.json to get vocab size
‚úÖ Builds character-level GRU model
‚úÖ Trains model on strong password patterns
‚úÖ Saves model as gru_base_rnn.h5 for generation
============================================================
"""

import os
import json
import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

# ============================================================
# üîπ CONFIGURATION
# ============================================================

SEQ_LEN = 24
BASE_DIR = r"D:\CSE\Project\KeyCrpyt\engine\scripts"  # ‚úÖ consistent path

# Preprocessed dataset + vocab + model save path
DATA_PATH = os.path.join(BASE_DIR, "kaggle_strong_passwords.csv")
VOCAB_PATH = os.path.join(BASE_DIR, "vocab.json")
MODEL_PATH = os.path.join(BASE_DIR, "gru_base_rnn.h5")

os.makedirs(BASE_DIR, exist_ok=True)

# ============================================================
# üîπ LOAD DATA
# ============================================================

if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"‚ùå {DATA_PATH} not found! Run preprocessing script first.")
print("üìÇ Loading dataset...")
df = pd.read_csv(DATA_PATH)

if "password" not in df.columns or "strength" not in df.columns:
    raise ValueError("‚ùå Dataset must contain 'password' and 'strength' columns")

# Filter only strong passwords (label == 2)
df_strong = df[df["strength"] == 2].dropna(subset=["password"])
passwords = df_strong["password"].astype(str).tolist()

print(f"‚úÖ Loaded {len(passwords)} strong passwords")

# ============================================================
# üîπ LOAD VOCABULARY
# ============================================================
print("üìÇ Loading vocabulary...")
if not os.path.exists(VOCAB_PATH) or os.path.getsize(VOCAB_PATH) == 0:
    raise FileNotFoundError(
        f"‚ùå vocab.json not found or is empty at {VOCAB_PATH}. "
        "Please run preprocess_strong_dataset.py first."
    )

try:
    with open(VOCAB_PATH, "r", encoding="utf-8") as f:
        vocab = json.load(f)
except json.JSONDecodeError:
    raise ValueError(
        f"‚ö†Ô∏è vocab.json at {VOCAB_PATH} is corrupted or invalid. "
        "Delete it and regenerate using preprocess_strong_dataset.py."
    )

vocab_size = len(vocab) + 1  # +1 for padding
char_to_idx = {char: idx for char, idx in vocab.items()}
print(f"üî§ Vocabulary size = {vocab_size}")

# ============================================================
# üîπ PREPARE TRAINING DATA
# ============================================================

print("üîÑ Preparing training sequences...")

def create_sequences(passwords, char_to_idx, seq_len):
    """
    Create input-output pairs for training.
    For each password, we create sliding windows of length seq_len
    and predict the next character.
    """
    X = []
    y = []

    for password in passwords:
        # Convert password to indices
        indices = [char_to_idx.get(char, 0) for char in password]

        # Create sequences
        for i in range(len(indices) - seq_len):
            sequence = indices[i:i + seq_len]
            target = indices[i + seq_len]
            X.append(sequence)
            y.append(target)

    return np.array(X), np.array(y)

# Generate training data
X, y = create_sequences(passwords, char_to_idx, SEQ_LEN)

print(f"‚úÖ Created {len(X)} training sequences")
print(f"üìä X shape: {X.shape}, y shape: {y.shape}")

# ============================================================
# üîπ BUILD GRU MODEL
# ============================================================

print("üß† Building GRU model...")

model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=128, input_length=SEQ_LEN),
    GRU(256, return_sequences=True),
    Dropout(0.3),
    GRU(256),
    Dropout(0.3),
    Dense(vocab_size, activation="softmax")
])

model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()

# ============================================================
# üîπ CALLBACKS
# ============================================================

checkpoint = ModelCheckpoint(
    MODEL_PATH,
    monitor="val_loss",
    save_best_only=True,
    verbose=1
)

lr_reducer = ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.5,
    patience=3,
    verbose=1,
    min_lr=1e-6
)

early_stop = EarlyStopping(
    monitor="val_loss",
    patience=7,
    restore_best_weights=True,
    verbose=1
)

# ============================================================
# üîπ TRAIN MODEL
# ============================================================

print("üöÄ Training GRU model on strong passwords...")
print(f"‚è±Ô∏è  This may take a while depending on dataset size...")

history = model.fit(
    X, y,
    validation_split=0.1,
    batch_size=256,
    epochs=50,
    callbacks=[checkpoint, lr_reducer, early_stop],
    verbose=1
)

print("‚úÖ Training completed successfully!")

# ============================================================
# üîπ SAVE FINAL MODEL & TRAINING HISTORY
# ============================================================

model.save(MODEL_PATH)
print(f"üíæ Model saved ‚Üí {MODEL_PATH}")

# Save training history
history_path = os.path.join(BASE_DIR, "training_history.json")
history_dict = {
    "loss": [float(x) for x in history.history["loss"]],
    "accuracy": [float(x) for x in history.history["accuracy"]],
    "val_loss": [float(x) for x in history.history["val_loss"]],
    "val_accuracy": [float(x) for x in history.history["val_accuracy"]]
}

with open(history_path, "w") as f:
    json.dump(history_dict, f, indent=2)

print(f"üìä Training history saved ‚Üí {history_path}")

# ============================================================
# üîπ TRAINING SUMMARY
# ============================================================

print("\n" + "="*60)
print("üìà TRAINING SUMMARY")
print("="*60)
print(f"Final Training Loss:     {history.history['loss'][-1]:.4f}")
print(f"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}")
print(f"Final Val Loss:          {history.history['val_loss'][-1]:.4f}")
print(f"Final Val Accuracy:      {history.history['val_accuracy'][-1]:.4f}")
print(f"Total Epochs Trained:    {len(history.history['loss'])}")
print("="*60)
print("\n‚ú® Model is ready for password generation!")
